{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9833c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae4bf8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# EPFL Course: CH-630 Drug Discovery\n",
    "\n",
    "## Doctoral School EDCH\n",
    "\n",
    "## Week 4: Exercises\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 style=\"color:green;\"> Lesson 4.5: Exercises on Ligand-Based Methods </h1>\n",
    "\n",
    "Ligand-based methods use information about known ligands to make predictions about new compounds. Instead of relying on the protein structure, these approaches analyze the chemical features, patterns and relationships between molecules themselves ‚Äî their shapes, fingerprints, physicochemical descriptors and measured activities ‚Äî to infer what makes a compound active or inactive.\n",
    "\n",
    "In this lessons we will explore:\n",
    "\n",
    "1. Basic cheminformatics with RDKit and molecular descriptors \n",
    "2. PCA for dimensionality reduction \n",
    "3. Simple machine learning regression (Random Forest) to predict a molecular property<br><br> \n",
    "\n",
    "> üí° This lesson will help you understand how ligand-based computational methods work, from computing molecular descriptors to exploring chemical space and building simple predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40596399",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\"> Step 1: Basic cheminformatics with RDKit </h2>\n",
    "\n",
    "In this first step, we will use RDKit, one of the most widely used open-source cheminformatics libraries. RDKit provides a complete toolkit for representing, manipulating, and analyzing molecular structures directly in Python.\n",
    "\n",
    "We will transform chemical structures ‚Äî typically stored as SMILES strings or SDF files ‚Äî into meaningful mathematical objects. This allows us to compute molecular fingerprints, molecular descriptors, evaluate similarity between molecules and prepare inputs for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 1.1: Load a small dataset </h3>\n",
    "\n",
    "We will use a small subset of the ESOL dataset, which contains experimental water solubility values expressed on a base-10 logarithmic scale (logS).\n",
    "\n",
    "Higher logS values correspond to greater water solubility, whereas lower (more negative) logS values indicate poor water solubility. Low solubility can lead to poor dissolution and may limit the bioavailability of a compound.\n",
    "\n",
    "Reference: *Delaney, 2004 https://pubs.acs.org/doi/10.1021/ci034243x.*\n",
    "\n",
    "This gives us a realistic property to predict later with machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound and real experimental solubility values (logS) from the ESOL dataset\n",
    "data = {\"Compound ID\": [\"1-Butene\", \"Ethanol\", \"Butane\", \"Butanethiol\", \"Benzene\", \"Pyridazine\", \"Pyridine\", \"Pyrimidine\",\n",
    "                        \"2-Iodopropane\", \"Dipropyl ether\", \"1,2-Dichloroethane\", \"1-Pentene\", \"2-Hydroxypyridine\", \"Acetamide\",\n",
    "                        \"Fluorobenzene\", \"Anisole\", \"Bromochloromethane\", \"Diethyl ether\", \"Ethane\", \"2-pyrrolidone\"],\n",
    "        \"SMILES\": [\"CCC=C\", \"CCO\", \"CCCC\", \"CCCCS\", \"c1ccccc1\", \"c1ccnnc1\", \"c1ccncc1\", \"c1cncnc1\", \"CC(C)I\", \"CCCOCCC\",\n",
    "                         \"ClCCCl\", \"CCCC=C\", \"Oc1ccccn1\", \"CC(=O)N\", \"Fc1ccccc1\", \"COc1ccccc1\", \"ClCBr\", \"CCOCC\", \"CC\", \"O=C1CCCN1\"],\n",
    "        \"logS_measured\": [-1.94, 1.1, -2.57, -2.18, -1.64, 1.1, 0.76, 1.1, -2.09, -1.62, -1.06, -2.68, 1.02, 1.58, -1.8, -1.85, -0.89, -0.09, -1.36, 1.07]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbbbe0",
   "metadata": {},
   "source": [
    "First of all, we will visualize the data in a dataset using the module `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 1.2: Convert molecules and compute descriptors </h3>\n",
    "\n",
    "Once we have a list of molecules represented as SMILES strings, we can use RDKit to convert each SMILES into an internal molecular object and compute a variety of basic physicochemical descriptors.\n",
    "\n",
    "These descriptors summarize important molecular properties and are widely used in cheminformatics and QSAR studies. In this exercise, we focus on a small set of simple but informative descriptors:\n",
    "\n",
    "- Molecular Weight (MW) ‚Äì total mass of the molecule\n",
    "\n",
    "- LogP ‚Äì predicted octanol/water partition coefficient (a measure of hydrophobicity)\n",
    "\n",
    "- HBD (Hydrogen Bond Donors) ‚Äì number of groups capable of donating H-bonds\n",
    "\n",
    "- HBA (Hydrogen Bond Acceptors) ‚Äì number of groups capable of accepting H-bonds\n",
    "\n",
    "- TPSA (Topological Polar Surface Area) ‚Äì a measure related to polarity and permeability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute molecular descriptors\n",
    "def compute_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return {\n",
    "        'MW': Descriptors.MolWt(mol),\n",
    "        'LogP': Descriptors.MolLogP(mol),\n",
    "        'HBD': Descriptors.NumHDonors(mol),\n",
    "        'HBA': Descriptors.NumHAcceptors(mol),\n",
    "        'TPSA': Descriptors.TPSA(mol)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af058f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute descriptors for each compound\n",
    "X = df['SMILES'].apply(compute_descriptors).apply(pd.Series)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d304b",
   "metadata": {},
   "source": [
    "We obtained the matrix $\\rm X$, which is a clean numeric feature matrix:\n",
    "\n",
    "- Each row = one molecule\n",
    "\n",
    "- Each column = one descriptor (MW, LogP, HBD, HBA, TPSA)\n",
    "\n",
    "This is the format required for data visualization (PCA) or machine learning (Random Forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\"> Step 2: PCA to explore chemical space </h2>\n",
    "\n",
    "Each molecule in the dataset is described by several numerical descriptors, including Molecular weight, logP, H-bond donors, H-bond acceptors, TPSA.\n",
    "\n",
    "In real drug-discovery projects, the number of descriptors can easily reach hundreds to thousands (fingerprints, structural fragments, physicochemical descriptors, 3D descriptors, etc.).\n",
    "\n",
    "This means each molecule corresponds to a point in a high-dimensional space: in our simple case 5D, but in real drug discovery cases also 100D-1000D.\n",
    "\n",
    "As humans cannot visualize high-dimensional data, we need a way to reduce the dimensionality while keeping most of the information.<br><br>\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a  linear dimensionality reduction technique that allows to visualize and explore data.\n",
    "With PCA, the dataset is transformed such that the directions capturing the largest variation in the data are easily identifiable.\n",
    "\n",
    "The steps are:\n",
    "- Identify directions in the dataset with maximum variance\n",
    "\n",
    "- Create new axes called principal components (PC1, PC2, ‚Ä¶)\n",
    "\n",
    "- Order them such that: PC1 captures the largest variance, PC2 captures the next largest, and so on.\n",
    "\n",
    "By keeping only the first two principal components (PC1 and PC2), we can plot molecules in 2D, providing a map of chemical space that highlights similarities, differences, and trends in molecular properties.<br><br>\n",
    "\n",
    "Consequently, PCA plots are often used in drug discovery for:\n",
    "\n",
    "- spotting chemical diversity\n",
    "\n",
    "- identifying outliers\n",
    "\n",
    "- comparing different chemical series\n",
    "\n",
    "- checking if a dataset is well-balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f907fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on the descriptor matrix, reducing the descriptor space to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# the high-dimensional data is projected onto these 2 principal components\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(f\"PC1: {X_pca[:,0]}\")\n",
    "print(f\"PC2: {X_pca[:,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b237b",
   "metadata": {},
   "source": [
    "With PCA, we compressed the high-dimensional descriptor space of our dataset into 2 dimensions while keeping the largest possible amount of information (i.e., variance).\n",
    "\n",
    "After reducing the descriptors to two principal components (PC1 and PC2), we can create a 2D scatter plot:\n",
    "\n",
    "- Each point represents one molecule\n",
    "\n",
    "- Proximity between points may indicate chemical similarity based on the descriptors used\n",
    "\n",
    "    -   Molecules that are close together may have similar molecular weight, logP, H-bond counts, TPSA\n",
    "\n",
    "    -   Molecules far apart may differ significantly in these properties\n",
    "\n",
    "- Molecules are color-coded by a property (logS in our case, but pKa, logP, pIC50 are also common). This allows us to see patterns and trends:\n",
    "\n",
    "    -   A gradient in color might reveal how the property changes with molecular structure\n",
    "\n",
    "    -   Clusters of molecules with similar properties may be identified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data can now plotted with a 2D scatter plot\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=df['logS_measured'], cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of molecular descriptors')\n",
    "plt.colorbar(label='logS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance captured by each principal component\n",
    "\n",
    "var_pc1 = np.var(X_pca[:, 0], ddof=1)\n",
    "var_pc2 = np.var(X_pca[:, 1], ddof=1)\n",
    "\n",
    "print(var_pc1, var_pc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f8173",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. PC1 often captures the majority of the variance in your dataset, so it should have a wider range than PC2. How are your data spreaded along PC1 and PC2?\n",
    "\n",
    "2. Are some points isolated? This means they have have descriptor values very different from the rest and could be outliers.\n",
    "\n",
    "3. The color bar shows logS (solubility) values. Molecules with similar colors should be relatively close together in PC space. Are extreme soluble molecules well-separated in the plot? Do you see a structure-property (logS) trend?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281a054",
   "metadata": {},
   "source": [
    "In conclusion, PCA helps in:\n",
    "\n",
    "- Visualizing chemical diversity in a dataset\n",
    "\n",
    "- Exploring structure‚Äìproperty relationships\n",
    "\n",
    "- Detecting potential outliers or artifacts\n",
    "\n",
    "- Preparing data for further analysis, like machine learning regression <br><br>\n",
    "\n",
    "However, it is important to remember that PCA does not create clusters by itself, it only allows us to visually explore data. Even if the first two components usually capture most of the variability, they do not capure it all, and some information is inevitably lost.\n",
    "\n",
    "It is also important to note that the PCs don't normally have a physical-chemical meaning, but represent a combination of the different properties.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\"> Step 3: Simple Machine Learning Regression </h2>\n",
    "\n",
    "Once we have represented molecules with numerical descriptors and explored the chemical space using PCA, we can try to predict a molecular property from these descriptors.\n",
    "\n",
    "**Machine learning regression** allows us to learn patterns between molecular features (like MW, LogP, H-bond counts, TPSA) and experimental properties (like solubility, logS). In this exercise, we will use a **Random Forest Regressor**, a robust and widely used ensemble method that combines multiple decision trees to improve prediction accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f8a10",
   "metadata": {},
   "source": [
    "The steps are:\n",
    "\n",
    "- Split the dataset into training and test sets;\n",
    "\n",
    "- Train the Random Forest model on the training data;\n",
    "\n",
    "- Predict the property for the test set and compare with true values;\n",
    "\n",
    "- Assess model performance using metrics like Mean Squared Error (MSE);\n",
    "\n",
    "- Inspect feature importance to understand which descriptors contribute most to the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb54aa4",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 3.1: Split the dataset into training and test sets </h3>\n",
    "\n",
    "- Before training any machine learning model, we need to separate the data we will use to learn patterns (training set) from the data we will use to evaluate the model (test set). If we evaluate the model on the same data it was trained on, it might appear to perform perfectly, but this can be misleading because the model could just memorize the training examples rather than learning general rules.\n",
    "\n",
    "- Testing the model on data that is not seen during training prevents overfitting, making sure the ML model is really learning patterns.\n",
    "\n",
    "- Typically, 70‚Äì80% of the data is used for training, and 20‚Äì30% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc498243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target of the model = logS\n",
    "y = df['logS_measured']\n",
    "\n",
    "# train/test split (70% test and 30% train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6e6de",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 3.2: Train the Random Forest model on the training data </h3>\n",
    "\n",
    "- The Random Forest algorithm builds an ensemble of decision trees. Each tree is trained on a random subset of the data and descriptors.\n",
    "\n",
    "- By averaging the predictions of many trees, Random Forest reduces errors and is less sensitive to noise or outliers.\n",
    "\n",
    "- During training, the model learns relationships between descriptors (MW, LogP, HBD, HBA, TPSA) and the target property (logS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8d201",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 3.3: Predict the property for the test set and compare with true values. </h3>\n",
    "Once trained, the model is used to predict logS for molecules it has not seen before (the test set).\n",
    "\n",
    "The prediction is compared with experimental values allows us to check how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79cdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted vs true\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')  # diagonal line\n",
    "plt.xlabel('True logS')\n",
    "plt.ylabel('Predicted logS')\n",
    "plt.title('Random Forest Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f622a1",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 3.4: Assess model performance </h3>\n",
    "\n",
    "- Comparing predictions with experimental values allows us to check how well the model generalizes.\n",
    "\n",
    "- The **mean squared error (MSE)** measures the average squared difference between predicted and true values.\n",
    "\n",
    "- Lower MSE means the model‚Äôs predictions are closer to experimental values.\n",
    "\n",
    "- The **coefficient of determination (R¬≤)** quantifies the fraction of variance in the experimental data explained by the model.\n",
    "\n",
    "- An R¬≤ value close to 1 indicates strong predictive performance, while values near 0 (or negative) indicate poor explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94007859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('R^2:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dde51c",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Step 3.5: Inspect feature importance </h3>\n",
    "\n",
    "Random Forest allows us to evaluate how much each descriptor contributes to the predictions.\n",
    "\n",
    "This can provide chemical insight: for example, if LogP or TPSA is highly important, it suggests solubility is strongly influenced by hydrophobicity or polarity.\n",
    "\n",
    "Feature importance is a useful way to interpret ‚Äúblack-box‚Äù machine learning models in a chemistry context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12077c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "importances = model.feature_importances_\n",
    "for desc, imp in zip(X.columns, importances):\n",
    "    print(f\"{desc}: {imp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75287db",
   "metadata": {},
   "source": [
    "1. How well does the model generalize to unseen molecules? Are the predictions for the test set close to the true values?\n",
    "\n",
    "2. Which descriptors are the most important for predicting solubility (logS) with this model?\n",
    "\n",
    "3. What chemical insight can you derive from the feature importance? For example, why might LogP or TPSA be particularly relevant?\n",
    "\n",
    "4. Are there any molecules where the prediction is particularly inaccurate? Can you explain why based on their descriptors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7fafb",
   "metadata": {},
   "source": [
    "<h2 style=\"color:orange;\"> Exercise </h2>\n",
    "\n",
    "- Try adding additional molecular properties or descriptors (e.g., number of rotatable bonds, aromatic proportion) and see how the model performance and feature importance change.\n",
    "\n",
    "- Experiment with different Random Forest hyperparameters (e.g., number of trees, maximum depth) and observe the effect on prediction accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
